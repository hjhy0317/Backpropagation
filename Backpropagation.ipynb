{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Backpropagation.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN+eB22rzBvv3Pw7vrjIf7n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GizC0bRU-HvO"},"source":["# **역전파**\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"ifj8s4neF3py"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bvoYD-9jG8PU"},"source":["import matplotlib.pyplot as plt\n","import matplotlib.image as img\n","\n","image = img.imread('/content/drive/MyDrive/ex.png')     # 실행 안 될 경우 4번 줄 주석처리하고 아래 5번 줄 주석처리(#) 제거하고 돌려보세요.\n","# image = img.imread('/content/drive/My Drive/ex.png')\n","plt.figure(figsize=(6, 5))\n","plt.imshow(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-2h4r-EalXx"},"source":["\n","\n","---\n","\n","\n","\n","> # **Numpy**"]},{"cell_type":"code","metadata":{"id":"mCtxg0LpZ_Zz"},"source":["import numpy as np\n","\n","# Create random input and output data\n","x = np.array([0.05, 0.10])\n","y = np.array([0.01, 0.99])\n","\n","# Randomly initialize weights\n","w1 = np.array([[0.15, 0.20], [0.25, 0.30]])\n","w2 = np.array([[0.40, 0.45], [0.50, 0.55]])\n","b1 = [0.35, 0.35]\n","b2 = [0.60, 0.60]\n","\n","learning_rate=0.01\n","for t in range(500):\n","    # Forward pass: compute predicted y\n","    h = x.dot(w1) + b1\n","    h_relu = np.maximum(h, 0)                   # 활성화 함수 : ReLU\n","    y_pred = h_relu.dot(w2) + b2                \n","\n","    # Compute and print loss\n","    loss = np.square(y_pred - y).sum()\n","    print(t, y_pred, '\\t', loss)\n","    # Backprop to compute gradients of w1\n","    grad_y_pred = 2.0 * (y_pred - y)\n","    grad_w2 = h_relu.T.dot(grad_y_pred)\n","    grad_h_relu = grad_y_pred.dot(w2.T)\n","    grad_h = grad_h_relu.copy()\n","    grad_h[h<0] = 0\n","    grad_w1 = x.T.dot(grad_h)\n","\n","    # Update weights\n","    w1 -= learning_rate * grad_w1\n","    w2 -= learning_rate * grad_w2\n","    b2 -= learning_rate * grad_y_pred\n","    b1 -= learning_rate * grad_h\n","\n","print('y_pred :', y_pred)\n","print('y : ', y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HkaMDHq52Sl-"},"source":["\n","\n","> # **nn.Module**\n","\n"]},{"cell_type":"code","metadata":{"id":"5omGxeCpj3p4"},"source":["import torch\n","import torch.nn as nn\n","from matplotlib import pyplot as plt\n","\n","torch.manual_seed(1)                            # model의 초기값 고정, 없애도 됩니다.\n","\n","x = torch.FloatTensor([0.05, 0.10])\n","y = torch.FloatTensor([0.01, 0.99])\n","\n","model = torch.nn.Sequential(\n","    nn.Linear(2, 2),\n","    nn.ReLU(),\n","    nn.Linear(2, 2)\n",")\n","criterion = torch.nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n","\n","epochs = 100\n","for epoch in range(epochs):\n","    y_pred = model(x)\n","    loss = criterion(y_pred, y)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    print(\"Epoch[{:5d}/{:5d}]   Cost: {:10f}  y_pred: {}\" .format(epoch, epochs, loss, y_pred.data.numpy()))\n","\n","print('y_pred : ', y_pred)\n","print('y : ', y)"],"execution_count":null,"outputs":[]}]}